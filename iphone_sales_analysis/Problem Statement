Spark Project -1 

Iphone Sales Analysis

Problem Statement
An iphone store wants to analyse the daily sales, Store has an application running which collects the data from user activities and 
pushes the sales data into partitioned hive table in parquet format. Store also has a metadata table in the hive which stores 
the product info. 

As a data expert you are supposed to work with multiple teams specially 2 teams one which is collecting the data 
from user activity(software engineers) and another which wants to analyse the collected data(Business data analyst or data scientist). 
So basically you are supposed to expose(write) to utilities.
 
Data collector utilities for software engineers
1.	Sales Data collector utility
a.	This utility will consume the sales data from a text file with header and will publish the output into a hive partitioned table 
in parquet format. You can assume sale_date as a partitioned column.
b.	So write a python function which will take the spark session and local file path as inputs and will return the name 
of the partitioned hive table.
i.	Assumptions
1.	Text file has metadata inform of file header.
2.	Data is separated by ‘|’
 
2.	Product Data collector utility
a.	Note - First convert text data into a parquet data for product
b.	This utility will consume the data from a parquet file and will publish into a non partitioned hive table.


Data preparation utilities for BDA or Data Scientist
1.	Write an utility which will consume the data from product and sales tables. And will publish the following output in another hive table.
2.	Expected output-
a.	Reports the buyers who have bought S8 but not iPhone. Note that S8 and iPhone are products present in the Product table 	





Metadata
Table: Product
+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| product_id   | int     |
| product_name | varchar |
| unit_price   | int     |
+--------------+---------+

1.	Product_id is the primary key of this table.
2.	Each row of this table indicates the name and the price of each product.

Table: Sales
+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| seller_id   | int     |
| product_id  | int     |
| buyer_id    | int     |
| sale_date   | date    |
| quantity    | int     |
| price       | int     |
 
+-------------+---------+
1.	This table has no primary key, it can have repeated rows.
2.	product_id is a foreign key to the Product table.
3.	buyer_id is never NULL. 
4.	sale_date is never NULL. 
5.	Each row of this table contains some information about one sale.



Sample Data

Input: 

Product table:
+------------+--------------+------------+
| product_id | product_name | unit_price |
+------------+--------------+------------+
| 1          | S8           | 1000       |
| 2          | G4           | 800        |
| 3          | iPhone       | 1400       |
+------------+--------------+------------+
Sales table:
+-----------+------------+----------+------------+----------+-------+
| seller_id | product_id | buyer_id | sale_date  | quantity | price |
+-----------+------------+----------+------------+----------+-------+
| 1         | 1          | 1        | 2019-01-21 | 2        | 2000  |
| 1         | 2          | 2        | 2019-02-17 | 1        | 800   |
| 2         | 1          | 3        | 2019-06-02 | 1        | 800   |
| 3         | 3          | 3        | 2019-05-13 | 2        | 2800  |
+-----------+------------+----------+------------+----------+-------+

Output: (Hive Table with single column) 
+-------------+
| buyer_id    |
+-------------+
| 1           |
+-------------+
Explanation: The buyer with id 1 bought an S8 but did not buy an iPhone. The buyer with id 3 bought both.


Submissions
1.	Submit your pySpark solution in a doc with the following utilities.
a.	sales_data_collector_api(spark, text_file_path)
b.	product_data_collector_api(spark, parquet_file_path)
c.	data_preparation_api(spark, product_hive_table, sales_hive_table, target_hive_table)

